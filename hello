from accelerate import Accelerator
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
from datetime import datetime

# Adjust Accelerator for multi-GPU distributed training
accelerator = Accelerator(gradient_accumulation_steps=1, mixed_precision='fp16', device_placement=True)

model = accelerator.prepare_model(model)  # This automatically handles multi-GPU placement

project = "llama3.2-finetune"
run_name = 'train-dir'
output_dir = "./" + run_name

args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=1,
    learning_rate=2.5e-5,  # Small lr for finetuning
    optim="paged_adamw_8bit",
    logging_steps=25,
    logging_dir="./logs",
    save_strategy="steps",
    save_steps=350,
    evaluation_strategy="steps",
    eval_steps=350,
    do_eval=True,
    num_train_epochs=3,
    fp16=True,  # Enables mixed precision
    dataloader_num_workers=4,
    report_to="tensorboard"
)

# Prepare datasets for distributed training
train_dataset, eval_dataset = accelerator.prepare(dataset['train'], dataset['test'])

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

trainer.train()
torchrun --nproc_per_node=NUM_GPUS script_name.py
